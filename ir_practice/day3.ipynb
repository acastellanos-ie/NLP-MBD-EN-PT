{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "day3.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PalIzNu-RsZn"
      },
      "source": [
        "# Information Retrieval Practice\n",
        "\n",
        "Elasticsearch is an open-source distributed search server built on top of Apache Lucene. Itâ€™s a great tool that allows to quickly build applications with full-text search capabilities. The core implementation is in Java, but it provides a nice REST interface which allows to interact with Elasticsearch from any programming language.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gJ3_XjRRsZq"
      },
      "source": [
        "## Install Elastic Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otODWy2xRsZr"
      },
      "source": [
        "To install elastic search download your the package for your platform from Get Elasticsearch\n",
        " in https://www.elastic.co/es/start\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7oP-ZAHRsZr"
      },
      "source": [
        "![](https://github.com/acastellanos-ie/natural_language_processing/blob/master/ir_practice/download.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEGrIn_eRsZs"
      },
      "source": [
        "Once downloaded, unzip the tar.gz file and run `bin/elasticsearch` (or `bin\\elasticsearch.bat` on Windows). This will launch the ElasticSearch Server. Once the server is running, by default it's accessible at [localhost:9200](http://localhost:9200)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFKFjUvNRsZs"
      },
      "source": [
        "## Querying Elastic Search via Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tabXiZwMRsZs"
      },
      "source": [
        "To make queries to ElasticSearch you can directly query the server endpoint via REST. However, we can make it easier via the the `elasticsearch-py` Python library. This library provides a wrapper for the REST endpoint that will allow us to query the server form Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEej2QJ1RsZt"
      },
      "source": [
        "from elasticsearch import Elasticsearch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeYSks38RsZt"
      },
      "source": [
        "# Exercise 0: Indexing and Searching Demo for ElasticSearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QUaDuuERsZu"
      },
      "source": [
        "Now it's time to run some demo program. In this practice, we will create inverted index of sample documents (indexing) and then use Elasticsearch query grammar to search documents (searching)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tscIKxuRsZu"
      },
      "source": [
        "### Useful functions\n",
        "\n",
        "Functions to facilitate the reading of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjZBcgm5RsZu"
      },
      "source": [
        "import os, io\n",
        "from collections import namedtuple\n",
        "\n",
        "Doc = namedtuple('Doc', 'filename path text')\n",
        "\n",
        "def read_doc(doc_path, encoding):\n",
        "    '''\n",
        "        reads a document from path\n",
        "        input:\n",
        "            - doc_path : path of document\n",
        "            - encoding: encoding\n",
        "        output: =>\n",
        "            - doc: instance of Doc namedtuple\n",
        "    '''\n",
        "    filename = doc_path.split('/')[-1]\n",
        "    fp = io.open(doc_path, 'r', encoding = encoding)\n",
        "    text = fp.read().strip()\n",
        "    fp.close()\n",
        "    return Doc(filename = filename, text = text, path = doc_path)\n",
        "\n",
        "def read_dataset(path, encoding = \"ISO-8859-1\"):\n",
        "    '''\n",
        "        reads multiple documents from path\n",
        "        input:\n",
        "            - doc_path : path of document\n",
        "            - encoding: encoding\n",
        "        output: =>\n",
        "            - docs: instances of Doc namedtuple returned as generator\n",
        "    '''\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for doc_path in files:\n",
        "            yield read_doc(root + '/' + doc_path, encoding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlpkewh5RsZv"
      },
      "source": [
        "## Indexing\n",
        "\n",
        "We will try to index the sample documents in `./sample_documents`. To index the documents, we first need to make a connection to **Elasticsearch**. \n",
        "\n",
        "Before we index the documents, we first need to define the **configuration of elasticsearch**. During this process, you can define basic configuration of indexer such as tokenizer, stemmer, lemmatizer, and also define which search algorithm elasticsearch will use for search.\n",
        "\n",
        "Below code shows a simple configuration settings for this demo.\n",
        "The configuration tells elasticsearch that our document `doc` will have three fields `filename`, `path`, and `text`, and we will use `text` field for search. `my_analyzer` will be used to parse the `text` field, and `my_analyzer` will also be used as a search analyzer, which will parse search queries later on. `index:False` in `filename` and `path` fields tell elasticsearch that we will not index these two fields, therefore, we cannot search these two fields with queries. \n",
        "\n",
        "The detailed documentation of analyzer can be found [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html).\n",
        "\n",
        "`\"similarity\": \"boolean\"` in `text` field will let elasticsearch know that we will use a boolean search algorithm to search `text` field. The detailed documentation of search algorithms can be found [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html)  and [here](https://www.elastic.co/guide/en/elasticsearch/guide/master/search-in-depth.html). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrURUGqTRsZy"
      },
      "source": [
        "# configuration for indexing\n",
        "settings = {\n",
        "  \"mappings\": {\n",
        "      \"properties\": {\n",
        "        \"filename\": {\n",
        "          \"type\": \"keyword\",\n",
        "          \"index\": False,\n",
        "        },\n",
        "        \"path\": {\n",
        "          \"type\": \"keyword\",\n",
        "          \"index\": False,\n",
        "        },\n",
        "        \"text\": {\n",
        "          \"type\": \"text\",\n",
        "          \"similarity\": \"boolean\",\n",
        "          \"analyzer\": \"my_analyzer\",\n",
        "          \"search_analyzer\": \"my_analyzer\"\n",
        "        }\n",
        "      }\n",
        "  },    \n",
        "  \"settings\": {      \n",
        "    \"analysis\": {\n",
        "      \"analyzer\": {\n",
        "        \"my_analyzer\": {\n",
        "          \"filter\": [\n",
        "            \"lowercase\",\"stop\"\n",
        "          ],\n",
        "          \"type\": \"custom\",\n",
        "          \"tokenizer\": \"whitespace\",\n",
        "          \"char_filter\": [\"my_char_filter\"]\n",
        "        }\n",
        "      },\n",
        "      \"char_filter\": {\n",
        "        \"my_char_filter\": {\n",
        "          \"type\": \"html_strip\",\n",
        "          \"escaped_tags\": [\"b\"]\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iiG4dDuRsZ0"
      },
      "source": [
        "Now we will retrieve `sample documents` and indexing them into `INDEX_NAME` index. To that end, the following 2 functions will help you in the creation of the index and the indexing of the documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [
          5,
          22
        ],
        "id": "QlzDMmrxRsZ1",
        "outputId": "0de750c8-8aa5-4b3c-cf1c-58d02010ad38"
      },
      "source": [
        "ES_HOSTS = ['http://localhost:9200']\n",
        "INDEX_NAME = 'sample_index'\n",
        "DOCS_PATH = 'practice_data/sample_documents'\n",
        "\n",
        "def create_index(es_conn, index_name, settings):\n",
        "    '''\n",
        "        create index structure in elasticsearch server. \n",
        "        If index_name exists in the server, it will be removed, and new index will be created.\n",
        "        input:\n",
        "            - es_conn: elasticsearch connection object\n",
        "            - index_name: name of index to create\n",
        "            - settings: settings and mappings for index to create\n",
        "        output: =>\n",
        "            - None\n",
        "    '''\n",
        "    if es_conn.indices.exists(index_name):\n",
        "        es_conn.indices.delete(index = index_name)\n",
        "        print('index `{}` deleted'.format(index_name))\n",
        "    es_conn.indices.create(index = index_name, body = settings)\n",
        "    print('index `{}` created'.format(index_name))            \n",
        "            \n",
        "def build_index(es_conn, dataset, index_name, settings, DOC_TYPE='doc'):\n",
        "    '''\n",
        "        build index from a collection of documents\n",
        "        input:\n",
        "            - es_conn: elasticsearch connection object\n",
        "            - dataset: iterable, collection of namedtuple Doc objects\n",
        "            - index_name: name of the index where the documents will be stored\n",
        "            - DOC_TYPE: type signature of documents\n",
        "    '''\n",
        "    # create the index if it doesn't exist\n",
        "    create_index(es_conn = es_conn, index_name = index_name, settings=settings)\n",
        "    counter_read, counter_idx_failed = 0, 0 # counters\n",
        "\n",
        "    # retrive & index documents\n",
        "    for doc in dataset:\n",
        "        res = es_conn.index(\n",
        "            index = index_name,\n",
        "            id = doc.filename,\n",
        "            body = doc._asdict())\n",
        "        counter_read += 1\n",
        "\n",
        "        if res['result'] != 'created':\n",
        "            counter_idx_failed += 1\n",
        "        elif counter_read % 500 == 0:\n",
        "            print('indexed {} documents'.format(counter_read))\n",
        "\n",
        "    print('indexed {} docs to index `{}`, failed to index {} docs'.format(\n",
        "        counter_read,\n",
        "        index_name,\n",
        "        counter_idx_failed\n",
        "    ))\n",
        "    \n",
        "    # refresh after indexing\n",
        "    es_conn.indices.refresh(index=index_name)  \n",
        "\n",
        "es_conn = Elasticsearch(ES_HOSTS)\n",
        "dataset = read_dataset(DOCS_PATH)\n",
        "build_index(es_conn, dataset, INDEX_NAME, settings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "index `sample_index` deleted\n",
            "index `sample_index` created\n",
            "indexed 5 docs to index `sample_index`, failed to index 0 docs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5r87By_RsZ3"
      },
      "source": [
        "We successfully created an inverted index for the sample documents in `./sample/documents`. It's time to search the documents with some queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvW8IkHGRsZ3"
      },
      "source": [
        "## Searching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6vMw7luRsZ3"
      },
      "source": [
        "**Elasticsearch** supports a specific query grammar which intends to replicate the grammar of traditional search engines (Google Search supports a similar grammar).\n",
        "https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html\n",
        "\n",
        "To understand score of the result, check: https://www.elastic.co/guide/en/elasticsearch/guide/current/relevance-intro.html#explain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz30NUoLRsZ3"
      },
      "source": [
        "### Useful Functions\n",
        "\n",
        "These functions will help you with the ElasticSearch output format in order to visualize the search results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7fktsEqRsZ4"
      },
      "source": [
        "def extract_response(res):\n",
        "    if res is not None:\n",
        "        for hit in res['hits']['hits']:\n",
        "            filename = hit[\"_source\"][\"filename\"]\n",
        "            score = hit[\"_score\"]\n",
        "            \n",
        "            yield (filename, score)\n",
        "\n",
        "def print_result(query, res):\n",
        "    # formatter of searched result\n",
        "    matches = extract_response(res)\n",
        "    if matches is not None:\n",
        "        for match in sorted(matches, key = lambda x: -x[1]):\n",
        "            print('{}, {}, {},\\n'.format(\n",
        "                query,\n",
        "                match[0], # filename\n",
        "                match[1], # score\n",
        "            ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vYJgLgfRsZ4"
      },
      "source": [
        "We will perform now different types of queries.\n",
        "\n",
        "First, a query with a single term"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV0mrVMxRsZ6",
        "outputId": "c5e9c679-e712-4792-8fb7-7ada3f682bc8"
      },
      "source": [
        "# Boolean Query \"Obama\"\n",
        "res = es_conn.search(index = INDEX_NAME,\n",
        "    body={\n",
        "          \"query\": {\n",
        "            \"bool\": {\n",
        "              \"must\": [\n",
        "                {\n",
        "                  \"match\": {\"text\": \"Obama\"}\n",
        "                }\n",
        "              ]\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "    )\n",
        "print_result(\"Obama\", res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obama, doc1.txt, 1.0,\n",
            "\n",
            "Obama, doc3.txt, 1.0,\n",
            "\n",
            "Obama, doc5.txt, 1.0,\n",
            "\n",
            "Obama, doc2.txt, 1.0,\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mjq0TWZbRsZ8"
      },
      "source": [
        "Now a query for the documents containing both terms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3GCQauZRsZ9",
        "outputId": "612c6d14-c684-4494-8238-e8e19fb00864"
      },
      "source": [
        "# Boolean Query \"Obama AND Hillary\"\n",
        "res = es_conn.search(index = INDEX_NAME,\n",
        "    body={\n",
        "          \"query\": {\n",
        "            \"match\" : {\n",
        "              \"text\" : {\n",
        "                \"query\" : \"Obama Hillary\",\n",
        "                \"operator\" : \"and\"\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "    )\n",
        "print_result(\"Obama AND Hillary\", res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obama AND Hillary, doc1.txt, 2.0,\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLLyIUACRsZ-"
      },
      "source": [
        "And now containing a term but NOT the other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3CajeCKRsZ-",
        "outputId": "4079807f-6cc7-449e-982f-c45ee85c409e"
      },
      "source": [
        "# Boolean Query \"Obama BUT Hillary\"\n",
        "res = es_conn.search(index = INDEX_NAME,\n",
        "    body={\n",
        "          \"query\": {\n",
        "            \"bool\": {\n",
        "              \"must\": [\n",
        "                {\n",
        "                    \"match\": {\"text\": \"Obama\"}\n",
        "                }\n",
        "              ],\n",
        "              \"must_not\":[\n",
        "                {\n",
        "                    \"match\": {\"text\": \"Hillary\"}\n",
        "                }\n",
        "              ]\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "    )\n",
        "print_result(\"Obama BUT Hillary\", res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obama BUT Hillary, doc3.txt, 1.0,\n",
            "\n",
            "Obama BUT Hillary, doc5.txt, 1.0,\n",
            "\n",
            "Obama BUT Hillary, doc2.txt, 1.0,\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eODbNPLSRsZ-"
      },
      "source": [
        "Finally, the default behaviour for queries with more than one term: OR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kddFUYjORsZ_",
        "outputId": "fe06f6f3-d36c-4f4d-f45a-645b9665b7ca"
      },
      "source": [
        "# Boolean Query \"Obama OR Hillary\"\n",
        "# default is OR\n",
        "res = es_conn.search(index = INDEX_NAME,\n",
        "    body={\n",
        "          \"query\": {\n",
        "            \"match\" : {\n",
        "              \"text\" : {\n",
        "                \"query\" : \"Obama Hillary\",\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "    )\n",
        "print_result(\"Obama OR Hillary\", res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obama OR Hillary, doc1.txt, 2.0,\n",
            "\n",
            "Obama OR Hillary, doc3.txt, 1.0,\n",
            "\n",
            "Obama OR Hillary, doc5.txt, 1.0,\n",
            "\n",
            "Obama OR Hillary, doc4.txt, 1.0,\n",
            "\n",
            "Obama OR Hillary, doc2.txt, 1.0,\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZHGQnHcRsZ_"
      },
      "source": [
        "# Exercise 1: Evaluating Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raLX8WzWRsaA"
      },
      "source": [
        "We will show how the retrieved result can be evaluated by **trec_eval** evaluation program.\n",
        "\n",
        "**trec_eval** is the standard software for evaluating search engines with test collections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfDR2IgtRsaA"
      },
      "source": [
        "## TREC_EVAL setup\n",
        "\n",
        "First, we need to install `trec_eval`. To install\n",
        "\n",
        "- go to `trec_eval-master` folder\n",
        "- run shell command `make` to create `trec_eval` binary file (If your are using Windows, you can install `make` from [here](http://gnuwin32.sourceforge.net/packages/make.htm))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg1n84oVRsaA"
      },
      "source": [
        "Next, check the `government` folder which contains three things:\n",
        "\n",
        "- A set of documents needed to be indexed, in the *documents* directory.\n",
        "    \n",
        "- A set of queries, also called 'topics', in *topics/gov.topics* file. The format of **.topic* file is \"query_id query_terms\". For example, the first line of 'air.topics' file is\n",
        "    \n",
        "    `1 mining gold silver coal`\n",
        "    \n",
        "    which means that the ID of query is *01* and the corresponding query is *mining gold silver coal*.\n",
        "\n",
        "- A set of judgements, saying which documents are relevant for each query, in the *qrels/gov.qrels* file. The format of **.qrels* file is \"query_id 0 document_name binary_relevance\". For example, the first line of 'air.qrels' is\n",
        "    \n",
        "    `1 0 G00-00-0681214 0`\n",
        "    \n",
        "    which means that the document `G00-00-0681214` is not relevant to the given query id *01*. The binary relevance is *1* if the file is relevant to the query, otherwise *0*. Please ignore the second argument *0* as it is always *0*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPLB1qMtRsaA"
      },
      "source": [
        "## Create new index\n",
        "\n",
        "In the previous exercise, we have created the index (inverted-index) of five sample documents. In this one, you will create a new index with the documents in `government/documents` folder .\n",
        "\n",
        "To build a new index, you first need to create a new index. Note that `EVAL_INDEX_NAME` should be changed in order to build separate index for the documents in `government/documents`.\n",
        "\n",
        "After creating the new configuration file, now your job is to create the new index reusing the code in the previous exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdmOF-4PRsaB"
      },
      "source": [
        "settings = {\n",
        "  \"mappings\": {\n",
        "      \"properties\": {\n",
        "        \"filename\": {\n",
        "          \"type\": \"keyword\",\n",
        "          \"index\": False,\n",
        "        },\n",
        "        \"path\": {\n",
        "          \"type\": \"keyword\",\n",
        "          \"index\": False,\n",
        "        },\n",
        "        \"text\": {\n",
        "          \"type\": \"text\",\n",
        "          \"similarity\": \"boolean\",\n",
        "          \"analyzer\": \"my_analyzer\",\n",
        "          \"search_analyzer\": \"my_analyzer\"\n",
        "        }\n",
        "      }\n",
        "  },    \n",
        "  \"settings\": {      \n",
        "    \"analysis\": {\n",
        "      \"analyzer\": {\n",
        "        \"my_analyzer\": {\n",
        "          \"filter\": [\n",
        "            \"stop\"\n",
        "          ],\n",
        "          \"char_filter\": [\n",
        "            \"html_strip\"\n",
        "          ],\n",
        "          \"type\": \"custom\",\n",
        "          \"tokenizer\": \"whitespace\"\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2wLKMl-RsaB"
      },
      "source": [
        "### Exercise 1.1: Create the new index\n",
        "\n",
        "You can reuse the previous code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1FpLG7_RsaB",
        "outputId": "56e5a5f6-74e7-47da-bfbf-66f456d56215"
      },
      "source": [
        "ES_HOSTS = ['http://localhost:9200']\n",
        "EVAL_INDEX_NAME = 'government'\n",
        "EVAL_DOCS_PATH = 'practice_data/government/documents'\n",
        "\n",
        "es_conn = Elasticsearch(ES_HOSTS)\n",
        "dataset = read_dataset(EVAL_DOCS_PATH)\n",
        "build_index(es_conn, dataset, EVAL_INDEX_NAME, settings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "index `government` deleted\n",
            "index `government` created\n",
            "indexed 500 documents\n",
            "indexed 1000 documents\n",
            "indexed 1500 documents\n",
            "indexed 2000 documents\n",
            "indexed 2500 documents\n",
            "indexed 3000 documents\n",
            "indexed 3500 documents\n",
            "indexed 4000 documents\n",
            "indexed 4079 docs to index `government`, failed to index 0 docs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxkaZCtjRsaC"
      },
      "source": [
        "### Exercise 1.2. Read topics and produce result file\n",
        "\n",
        "Read topics (queries) from a file (`government/topics/gov.topics`) and then search documents indexed by **Elasticsearch**. You may choose one of search algorithms.\n",
        "\n",
        "Produce result file (e.g., *retrieved.txt*) according to **trec_eval** standard output format: \n",
        "\n",
        "`01 Q0 document1 0 1.23 my_IR_system1`\n",
        "\n",
        "`01 Q0 document2 1 1.08 my_IR_system1`\n",
        "\n",
        "where '01' is the query ID; ignore 'Q0'; 'documentX' is the name of the file; '0' (or '1' or some other integer number) is the rank of this result; '1.23' (or '1.08' or some other number) is the score of this result; and 'my_IR_system1' is the name for your retrieval system. In particular, note that the rank field will be ignored in **trec_eval**; internally ranks are assigned by sorting by the score field with ties broken deterministicly (using file name).\n",
        "\n",
        "**Now here's your first job**\n",
        "\n",
        "1. read `gov.topics` file line by line, \n",
        "2. send query to the elastic search\n",
        "3. write output according the the output format described above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ze2ToACbRsaC"
      },
      "source": [
        "def read_topic_file(path, encoding = \"ISO-8859-1\"):\n",
        "    '''\n",
        "        reads multiple documents from path\n",
        "        input:\n",
        "            - doc_path : path of document\n",
        "            - encoding: encoding\n",
        "        output: =>\n",
        "            - docs: instances of Doc namedtuple returned as generator\n",
        "    '''\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for doc_path in files:\n",
        "            filename = (root + '/' + doc_path)\n",
        "            fp = io.open(filename, 'r', encoding = encoding)\n",
        "            text = fp.readlines()\n",
        "            fp.close()\n",
        "    return [(t.split(\" \")[0].strip(), \" \".join(t.split(\" \")[1:]).strip()) for t in text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFz6YwlKRsaC"
      },
      "source": [
        "queries = read_topic_file(\"./practice_data/government/topics\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3QMdtWvRsaC",
        "outputId": "f8bbc022-9649-4014-d35b-88c0643700b0"
      },
      "source": [
        "queries"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1', 'mining gold silver coal'),\n",
              " ('2', 'juvenile delinquency'),\n",
              " ('4', 'wireless communications'),\n",
              " ('6', 'physical therapists'),\n",
              " ('7', 'cotton industry'),\n",
              " ('9', 'genealogy searches'),\n",
              " ('10', 'Physical Fitness'),\n",
              " ('14', 'Agricultural biotechnology'),\n",
              " ('16', 'Emergency and disaster preparedness assistance'),\n",
              " ('18', 'Shipwrecks'),\n",
              " ('19', 'Cybercrime, internet fraud, and cyber fraud'),\n",
              " ('22', \"Veteran's Benefits\"),\n",
              " ('24', 'Air Bag Safety'),\n",
              " ('26', 'Nuclear power plants'),\n",
              " ('28', 'Early Childhood Education')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9s3gsLsRsaD"
      },
      "source": [
        "def search(query_string, es_conn, index_name, operator = \"or\"):\n",
        "    '''\n",
        "        searches for query_string with default search algorithm\n",
        "        input:\n",
        "            - query_string: a query\n",
        "            - es_conn: elasticsearch connection\n",
        "            - index_name: name of index\n",
        "        output:\n",
        "            - a generator of tuple (filename, score)\n",
        "\n",
        "    '''\n",
        "    res = es_conn.search(index = index_name, size = 100,\n",
        "        body = {\n",
        "            \"_source\": [\"filename\"],\n",
        "            \"query\": {\n",
        "                \"match\": {\n",
        "                    \"text\":{\n",
        "                        \"query\": query_string,\n",
        "                        \"operator\" : operator    \n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    )\n",
        "    for hit in res['hits']['hits']:\n",
        "        filename = hit[\"_source\"][\"filename\"]\n",
        "        score = hit[\"_score\"]\n",
        "        yield (filename, score)\n",
        "\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_QlNjB1RsaD"
      },
      "source": [
        "def write_trec_file(query, res, output_file):\n",
        "    # formatter of searched result\n",
        "    for ranking, match in enumerate(sorted(res, key = lambda x: -x[1])):\n",
        "        output_file.write('{} Q0 {} {} {} {}\\n'.format(\n",
        "            query,\n",
        "            match[0], # filename\n",
        "            ranking,\n",
        "            match[1], # score\n",
        "            \"IR_system\"\n",
        "        ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi_BOZ5FRsaD"
      },
      "source": [
        "output_file = open(\"retrieved.txt\",\"w+\")\n",
        "\n",
        "es_conn = Elasticsearch(ES_HOSTS)\n",
        "for query_id, query in queries:\n",
        "    res = search(query, es_conn, EVAL_INDEX_NAME)\n",
        "    write_trec_file(query_id, res, output_file)\n",
        "    \n",
        "output_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qj6-mWsRsaE"
      },
      "source": [
        "### Exercise 1.3.  Evaluation\n",
        "\n",
        "It's time to run **trec_eval** which compares the qrels file provided in *gov.qrels* with your result file. (hint: adding a **!** and shell commands allow you to execute shell commands in jupyter-notebook, e.g. `!ls`)\n",
        "\n",
        "TREC_EVAL will evaluate the performance of your search engine. To evaluate your search result, you first need two sets of files: the retrieved result file and the ground truth file.\n",
        "Let's say your retrieval result is saved at `retrieved.txt`, and the ground truth file is saved at `gov.qrels`. The performance of your retrieval can be measured via:\n",
        "\n",
        "```\n",
        "./trec_eval.9.0/trec_eval  gov.qrels retrieved.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNmo277ORsaF",
        "outputId": "10f27c99-95e8-46af-d0eb-78edab11d915"
      },
      "source": [
        "! ./trec_eval-master/trec_eval ./practice_data/government/qrels/gov.qrels retrieved.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "runid                 \tall\tIR_system\n",
            "num_q                 \tall\t15\n",
            "num_ret               \tall\t1237\n",
            "num_rel               \tall\t35\n",
            "num_rel_ret           \tall\t17\n",
            "map                   \tall\t0.1121\n",
            "gm_map                \tall\t0.0086\n",
            "Rprec                 \tall\t0.0667\n",
            "bpref                 \tall\t0.0667\n",
            "recip_rank            \tall\t0.1235\n",
            "iprec_at_recall_0.00  \tall\t0.1287\n",
            "iprec_at_recall_0.10  \tall\t0.1287\n",
            "iprec_at_recall_0.20  \tall\t0.1287\n",
            "iprec_at_recall_0.30  \tall\t0.1228\n",
            "iprec_at_recall_0.40  \tall\t0.1220\n",
            "iprec_at_recall_0.50  \tall\t0.1133\n",
            "iprec_at_recall_0.60  \tall\t0.1080\n",
            "iprec_at_recall_0.70  \tall\t0.1080\n",
            "iprec_at_recall_0.80  \tall\t0.0989\n",
            "iprec_at_recall_0.90  \tall\t0.0989\n",
            "iprec_at_recall_1.00  \tall\t0.0989\n",
            "P_5                   \tall\t0.0133\n",
            "P_10                  \tall\t0.0267\n",
            "P_15                  \tall\t0.0356\n",
            "P_20                  \tall\t0.0333\n",
            "P_30                  \tall\t0.0267\n",
            "P_100                 \tall\t0.0113\n",
            "P_200                 \tall\t0.0057\n",
            "P_500                 \tall\t0.0023\n",
            "P_1000                \tall\t0.0011\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VVK8yDARsaF"
      },
      "source": [
        "# Improving the index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1HllsZPRsaG"
      },
      "source": [
        "The baseline retrieval that we have proposed before did offer a rather low performance. In order to improve it, we can tune the index setting to include some of the NLP processing that we have learned (e.g., stemming, stopwords, ...)-\n",
        "\n",
        "To that end, review the documentation of analyzer [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zw7QguqRsaG"
      },
      "source": [
        "Although we could generate our own analyzers (as we did in the previous exercises with `my_analyzer`), Elasticsearch provides a set of predefined analyzers for the different languages. More information [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html).\n",
        "\n",
        "In particular, we are going to use the `English Analyzer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKOkr5r0RsaG"
      },
      "source": [
        "In addition, we can modify the index to use a more sophisticated similarity measure (e.g., `BM25`) than the binary similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsyXl1YnRsaG"
      },
      "source": [
        "## Exercise 2.1 English Analyzer + BM25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xFN_B8nRsaG"
      },
      "source": [
        "Modify the settings to apply the `English Analyzer` and use the `BM25` similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmwaZYi2RsaH"
      },
      "source": [
        "new_settings = {  \n",
        "# Your code here\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiDOfeK0RsaH"
      },
      "source": [
        "With this new settings we will create a new index, generate a new result file and evaluate it by means of the `trec_eval`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgBGHnbqRsaH"
      },
      "source": [
        "ES_HOSTS = ['http://localhost:9200']\n",
        "EVAL_INDEX_NAME = 'government'\n",
        "EVAL_DOCS_PATH = 'practice_data/government/documents'\n",
        "\n",
        "es_conn = Elasticsearch(ES_HOSTS)\n",
        "dataset = read_dataset(EVAL_DOCS_PATH)\n",
        "build_index(es_conn, dataset, EVAL_INDEX_NAME, new_settings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ltbz3mtRRsaH"
      },
      "source": [
        "output_file = open(\"improved_retrieved.txt\",\"w+\")\n",
        "\n",
        "es_conn = Elasticsearch(ES_HOSTS)\n",
        "for query_id, query in queries:\n",
        "    res = search(query, es_conn, EVAL_INDEX_NAME)\n",
        "    write_trec_file(query_id, res, output_file)\n",
        "\n",
        "output_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDKRJrKlRsaH"
      },
      "source": [
        "!./trec_eval-master/trec_eval ./practice_data/government/qrels/gov.qrels improved_retrieved.txt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}