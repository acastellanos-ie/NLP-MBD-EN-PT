{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "day2.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw1gAgBrPKvO"
      },
      "source": [
        "# Information Retrieval Practice\n",
        "\n",
        "Elasticsearch is an open-source distributed search server built on top of Apache Lucene. Itâ€™s a great tool that allows to quickly build applications with full-text search capabilities. The core implementation is in Java, but it provides a nice REST interface which allows to interact with Elasticsearch from any programming language.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luC5jU7TPKvP"
      },
      "source": [
        "## Install Elastic Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRESTafqPKvR"
      },
      "source": [
        "To install elastic search download your the package for your platform from Get Elasticsearch\n",
        " in https://www.elastic.co/es/start\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BZ_Ksp1PKvR"
      },
      "source": [
        "![](https://github.com/acastellanos-ie/natural_language_processing/blob/master/ir_practice/download.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M61QkM18PKvR"
      },
      "source": [
        "Once downloaded, unzip the tar.gz file and run `bin/elasticsearch` (or `bin\\elasticsearch.bat` on Windows). This will launch the ElasticSearch Server. Once the server is running, by default it's accessible at [localhost:9200](http://localhost:9200)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTTWbBMjPKvS"
      },
      "source": [
        "## Querying Elastic Search via Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsk2gAqZPKvS"
      },
      "source": [
        "To make queries to ElasticSearch you can directly query the server endpoint via REST. However, we can make it easier via the the `elasticsearch-py` Python library. This library provides a wrapper for the REST endpoint that will allow us to query the server form Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfNJxrnJPKvS"
      },
      "source": [
        "from elasticsearch import Elasticsearch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSpOytV1PKvT"
      },
      "source": [
        "# Exercise 0: Indexing and Searching Demo for ElasticSearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYF55BAiPKvT"
      },
      "source": [
        "Now it's time to run some demo program. In this practice, we will create inverted index of sample documents (indexing) and then use Elasticsearch query grammar to search documents (searching)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljNaSxziPKvU"
      },
      "source": [
        "### Useful functions\n",
        "\n",
        "Functions to facilitate the reading of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLKYRs-gPKvV"
      },
      "source": [
        "import os, io\n",
        "from collections import namedtuple\n",
        "\n",
        "# A document class with following attributes\n",
        "# filename: document filename\n",
        "# text: body of documment\n",
        "# path: path of document\n",
        "Doc = namedtuple('Doc', 'filename path text')\n",
        "\n",
        "def read_doc(doc_path, encoding):\n",
        "    '''\n",
        "        reads a document from path\n",
        "        input:\n",
        "            - doc_path : path of document\n",
        "            - encoding: encoding\n",
        "        output: =>\n",
        "            - doc: instance of Doc namedtuple\n",
        "    '''\n",
        "    filename = doc_path.split('/')[-1]\n",
        "    fp = io.open(doc_path, 'r', encoding = encoding)\n",
        "    text = fp.read().strip()\n",
        "    fp.close()\n",
        "    return Doc(filename = filename, text = text, path = doc_path)\n",
        "\n",
        "def read_dataset(path, encoding = \"ISO-8859-1\"):\n",
        "    '''\n",
        "        reads multiple documents from path\n",
        "        input:\n",
        "            - doc_path : path of document\n",
        "            - encoding: encoding\n",
        "        output: =>\n",
        "            - docs: instances of Doc namedtuple returned as generator\n",
        "    '''\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for doc_path in files:\n",
        "            yield read_doc(root + '/' + doc_path, encoding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTDlFkyVPKvW"
      },
      "source": [
        "##  Indexing\n",
        "\n",
        "We will try to index the sample documents in `./sample_documents`. To index the documents, we first need to make a connection to **Elasticsearch**. \n",
        "\n",
        "Before we index the documents, we first need to define the **configuration of elasticsearch**. During this process, you can define basic configuration of indexer such as tokenizer, stemmer, lemmatizer, and also define which search algorithm elasticsearch will use for search.\n",
        "\n",
        "Below code shows a simple configuration settings for this demo.\n",
        "The configuration tells elasticsearch that our document `doc` will have three fields `filename`, `path`, and `text`, and we will use `text` field for search. `my_analyzer` will be used to parse the `text` field, and `my_analyzer` will also be used as a search analyzer, which will parse search queries later on. `index:False` in `filename` and `path` fields tell elasticsearch that we will not index these two fields, therefore, we cannot search these two fields with queries. \n",
        "\n",
        "The detailed documentation of analyzer can be found [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html).\n",
        "\n",
        "`\"similarity\": \"boolean\"` in `text` field will let elasticsearch know that we will use a boolean search algorithm to search `text` field. The detailed documentation of search algorithms can be found [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html)  and [here](https://www.elastic.co/guide/en/elasticsearch/guide/master/search-in-depth.html). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di4SZOXYPKvX"
      },
      "source": [
        "# configuration for indexing\n",
        "settings = {\n",
        "  \"mappings\": {\n",
        "      \"properties\": {\n",
        "        \"filename\": {\n",
        "          \"type\": \"keyword\",\n",
        "          \"index\": False,\n",
        "        },\n",
        "        \"path\": {\n",
        "          \"type\": \"keyword\",\n",
        "          \"index\": False,\n",
        "        },\n",
        "        \"text\": {\n",
        "          \"type\": \"text\",\n",
        "          \"similarity\": \"boolean\",\n",
        "          \"analyzer\": \"my_analyzer\",\n",
        "          \"search_analyzer\": \"my_analyzer\"\n",
        "        }\n",
        "      }\n",
        "  },    \n",
        "  \"settings\": {      \n",
        "    \"analysis\": {\n",
        "      \"analyzer\": {\n",
        "        \"my_analyzer\": {\n",
        "          \"filter\": [\n",
        "            \"lowercase\",\"stop\"\n",
        "          ],\n",
        "          \"type\": \"custom\",\n",
        "          \"tokenizer\": \"whitespace\",\n",
        "          \"char_filter\": [\"my_char_filter\"]\n",
        "        }\n",
        "      },\n",
        "      \"char_filter\": {\n",
        "        \"my_char_filter\": {\n",
        "          \"type\": \"html_strip\",\n",
        "          \"escaped_tags\": [\"b\"]\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuGAKaF3PKvY"
      },
      "source": [
        "Now we will retrieve `sample documents` and indexing them into `INDEX_NAME` index. To that end, the following 2 functions will help you in the creation of the index and the indexing of the documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "f9Vgrhe4PKvZ",
        "outputId": "c221f3d1-9a45-44cc-f154-27adfeaac467"
      },
      "source": [
        "ES_HOSTS = ['http://localhost:9200']\n",
        "INDEX_NAME = 'sample_index'\n",
        "DOCS_PATH = 'practice_data/sample_documents'\n",
        "\n",
        "def create_index(es_conn, index_name, settings):\n",
        "    '''\n",
        "        create index structure in elasticsearch server. \n",
        "        If index_name exists in the server, it will be removed, and new index will be created.\n",
        "        input:\n",
        "            - es_conn: elasticsearch connection object\n",
        "            - index_name: name of index to create\n",
        "            - settings: settings and mappings for index to create\n",
        "        output: =>\n",
        "            - None\n",
        "    '''\n",
        "    if es_conn.indices.exists(index_name):\n",
        "        es_conn.indices.delete(index = index_name)\n",
        "        print('index `{}` deleted'.format(index_name))\n",
        "    es_conn.indices.create(index = index_name, body = settings)\n",
        "    print('index `{}` created'.format(index_name))            \n",
        "            \n",
        "def build_index(es_conn, dataset, index_name, settings, DOC_TYPE='doc'):\n",
        "    '''\n",
        "        build index from a collection of documents\n",
        "        input:\n",
        "            - es_conn: elasticsearch connection object\n",
        "            - dataset: iterable, collection of namedtuple Doc objects\n",
        "            - index_name: name of the index where the documents will be stored\n",
        "            - DOC_TYPE: type signature of documents\n",
        "    '''\n",
        "    # create the index if it doesn't exist\n",
        "    create_index(es_conn = es_conn, index_name = index_name, settings=settings)\n",
        "    counter_read, counter_idx_failed = 0, 0 # counters\n",
        "\n",
        "    # retrive & index documents\n",
        "    for doc in dataset:\n",
        "        res = es_conn.index(\n",
        "            index = index_name,\n",
        "            id = doc.filename,\n",
        "            body = doc._asdict())\n",
        "        counter_read += 1\n",
        "\n",
        "        if res['result'] != 'created':\n",
        "            counter_idx_failed += 1\n",
        "        elif counter_read % 500 == 0:\n",
        "            print('indexed {} documents'.format(counter_read))\n",
        "\n",
        "    print('indexed {} docs to index `{}`, failed to index {} docs'.format(\n",
        "        counter_read,\n",
        "        index_name,\n",
        "        counter_idx_failed\n",
        "    ))\n",
        "    \n",
        "    # refresh after indexing\n",
        "    es_conn.indices.refresh(index=index_name)  \n",
        "\n",
        "es_conn = Elasticsearch(ES_HOSTS)\n",
        "dataset = read_dataset(DOCS_PATH)\n",
        "build_index(es_conn, dataset, INDEX_NAME, settings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "index `sample_index` deleted\n",
            "index `sample_index` created\n",
            "indexed 1 documents\n",
            "indexed 2 documents\n",
            "indexed 3 documents\n",
            "indexed 4 documents\n",
            "indexed 5 documents\n",
            "indexed 5 docs to index `sample_index`, failed to index 0 docs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoEy2rShPKva"
      },
      "source": [
        "We successfully created an inverted index for the sample documents in `./sample/documents`. It's time to search the documents with some queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8aPr1uRPKva"
      },
      "source": [
        "## Searching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G6D8B2uPKva"
      },
      "source": [
        "**Elasticsearch** supports a specific query grammar which intends to replicate the grammar of traditional search engines (Google Search supports a similar grammar).\n",
        "https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html\n",
        "\n",
        "To understand score of the result, check: https://www.elastic.co/guide/en/elasticsearch/guide/current/relevance-intro.html#explain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD4XrHKsPKva"
      },
      "source": [
        "### Useful Functions\n",
        "\n",
        "These functions will help you with the ElasticSearch output format in order to visualize the search results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mZl-Vz2PKvb"
      },
      "source": [
        "def extract_response(res):\n",
        "    if res is not None:\n",
        "        for hit in res['hits']['hits']:\n",
        "            filename = hit[\"_source\"][\"filename\"]\n",
        "            score = hit[\"_score\"]\n",
        "            \n",
        "            yield (filename, score)\n",
        "\n",
        "def print_result(query, res):\n",
        "    # formatter of searched result\n",
        "    matches = extract_response(res)\n",
        "    if matches is not None:\n",
        "        for match in sorted(matches, key = lambda x: -x[1]):\n",
        "            print('{}, {}, {},\\n'.format(\n",
        "                query,\n",
        "                match[0], # filename\n",
        "                match[1], # score\n",
        "            ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kOyvCgDPKvb"
      },
      "source": [
        "We will perform now different types of queries.\n",
        "\n",
        "First, a query with a single term"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4urvDw-lPKvb",
        "outputId": "05ad2e69-3c29-4c2b-cba1-a220f869ef03"
      },
      "source": [
        "res = es_conn.search(index = INDEX_NAME,\n",
        "    body={\n",
        "          \"query\": {\n",
        "            \"bool\": {\n",
        "              \"must\": [\n",
        "                {\n",
        "                  \"match\": {\"text\": \"Obama\"}\n",
        "                }\n",
        "              ]\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "    )\n",
        "print_result(\"Obama\", res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obama, doc1.txt, 1.0,\n",
            "\n",
            "Obama, doc3.txt, 1.0,\n",
            "\n",
            "Obama, doc5.txt, 1.0,\n",
            "\n",
            "Obama, doc2.txt, 1.0,\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfdLsy26PKvc"
      },
      "source": [
        "Now a query for the documents containing both terms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XieowoNNPKvf",
        "outputId": "3b5cfa51-939d-4d20-ef35-3e0717f3d0cb"
      },
      "source": [
        "# Boolean Query \"Obama AND Hillary\"\n",
        "res = es_conn.search(index = INDEX_NAME,\n",
        "    body={\n",
        "          \"query\": {\n",
        "            \"match\" : {\n",
        "              \"text\" : {\n",
        "                \"query\" : \"Obama Hillary\",\n",
        "                \"operator\" : \"and\"\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "    )\n",
        "print_result(\"Obama AND Hillary\", res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obama AND Hillary, doc1.txt, 2.0,\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6JyaWcIPKvf"
      },
      "source": [
        "And now containing a term but NOT the other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmBSy8tEPKvg",
        "outputId": "b6bac4a9-d428-43e5-d817-ae8fbfab5a4d"
      },
      "source": [
        "# Boolean Query \"Obama BUT Hillary\"\n",
        "res = es_conn.search(index = INDEX_NAME,\n",
        "    body={\n",
        "          \"query\": {\n",
        "            \"bool\": {\n",
        "              \"must\": [\n",
        "                {\n",
        "                    \"match\": {\"text\": \"Obama\"}\n",
        "                }\n",
        "              ],\n",
        "              \"must_not\":[\n",
        "                {\n",
        "                    \"match\": {\"text\": \"Hillary\"}\n",
        "                }\n",
        "              ]\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "    )\n",
        "print_result(\"Obama BUT Hillary\", res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obama BUT Hillary, doc3.txt, 1.0,\n",
            "\n",
            "Obama BUT Hillary, doc5.txt, 1.0,\n",
            "\n",
            "Obama BUT Hillary, doc2.txt, 1.0,\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JljEOJRSPKvg"
      },
      "source": [
        "Finally, the default behaviour for queries with more than one term: OR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lOy6p2MPKvh",
        "outputId": "c475c8df-c74b-4a76-c119-62b354071bed"
      },
      "source": [
        "# Boolean Query \"Obama OR Hillary\"\n",
        "# default is OR\n",
        "res = es_conn.search(index = INDEX_NAME,\n",
        "    body={\n",
        "          \"query\": {\n",
        "            \"match\" : {\n",
        "              \"text\" : {\n",
        "                \"query\" : \"Obama Hillary\",\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "    )\n",
        "print_result(\"Obama OR Hillary\", res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obama OR Hillary, doc1.txt, 2.0,\n",
            "\n",
            "Obama OR Hillary, doc3.txt, 1.0,\n",
            "\n",
            "Obama OR Hillary, doc5.txt, 1.0,\n",
            "\n",
            "Obama OR Hillary, doc4.txt, 1.0,\n",
            "\n",
            "Obama OR Hillary, doc2.txt, 1.0,\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq_rgKpgPKvh"
      },
      "source": [
        "# Exercise 1: Evaluating Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrtFRppnPKvh"
      },
      "source": [
        "We will show how the retrieved result can be evaluated by **trec_eval** evaluation program.\n",
        "\n",
        "**trec_eval** is the standard software for evaluating search engines with test collections. For downloading the latest version, please check the following [link](https://trec.nist.gov/trec_eval/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq-bffTFPKvi"
      },
      "source": [
        "## TREC_EVAL setup\n",
        "\n",
        "First, we need to install `trec_eval`. To install\n",
        "\n",
        "- unzip `trec_eval-master.zip`\n",
        "- go to `trec_eval-master` folder\n",
        "- run shell command `make` to create `trec_eval` binary file (If your are using Windows, you can install `make` from [here](http://gnuwin32.sourceforge.net/packages/make.htm), for more details on the Windows installation, you can check the official [trec_eval repo](https://github.com/usnistgov/trec_eval/blob/master/README.windows.md))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEVoLeHAPKvi"
      },
      "source": [
        "Next, check the `government` folder which contains three things:\n",
        "\n",
        "- A set of documents needed to be indexed, in the *documents* directory.\n",
        "    \n",
        "- A set of queries, also called 'topics', in *topics/gov.topics* file. The format of **.topic* file is \"query_id query_terms\". For example, the first line of 'air.topics' file is\n",
        "    \n",
        "    `1 mining gold silver coal`\n",
        "    \n",
        "    which means that the ID of query is *01* and the corresponding query is *mining gold silver coal*.\n",
        "\n",
        "- A set of judgements, saying which documents are relevant for each query, in the *qrels/gov.qrels* file. The format of **.qrels* file is \"query_id 0 document_name binary_relevance\". For example, the first line of 'air.qrels' is\n",
        "    \n",
        "    `1 0 G00-00-0681214 0`\n",
        "    \n",
        "    which means that the document `G00-00-0681214` is not relevant to the given query id *01*. The binary relevance is *1* if the file is relevant to the query, otherwise *0*. Please ignore the second argument *0* as it is always *0*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TlbXHwkPKvi"
      },
      "source": [
        "## Create new index\n",
        "\n",
        "In the previous exercise, we have created the index (inverted-index) of five sample documents. In this one, you will create a new index with the documents in `government/documents` folder .\n",
        "\n",
        "To build a new index, you first need to create a new index. Note that `EVAL_INDEX_NAME` should be changed in order to build separate index for the documents in `government/documents`.\n",
        "\n",
        "After creating the new configuration file, now your job is to create the new index reusing the code in the previous exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbsZiKdsPKvi"
      },
      "source": [
        "settings = {\n",
        "  \"mappings\": {\n",
        "      \"properties\": {\n",
        "        \"filename\": {\n",
        "          \"type\": \"keyword\",\n",
        "          \"index\": False,\n",
        "        },\n",
        "        \"path\": {\n",
        "          \"type\": \"keyword\",\n",
        "          \"index\": False,\n",
        "        },\n",
        "        \"text\": {\n",
        "          \"type\": \"text\",\n",
        "          \"similarity\": \"boolean\",\n",
        "          \"analyzer\": \"my_analyzer\",\n",
        "          \"search_analyzer\": \"my_analyzer\"\n",
        "        }\n",
        "      }\n",
        "  },    \n",
        "  \"settings\": {      \n",
        "    \"analysis\": {\n",
        "      \"analyzer\": {\n",
        "        \"my_analyzer\": {\n",
        "          \"filter\": [\n",
        "            \"stop\"\n",
        "          ],\n",
        "          \"char_filter\": [\n",
        "            \"html_strip\"\n",
        "          ],\n",
        "          \"type\": \"custom\",\n",
        "          \"tokenizer\": \"whitespace\"\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R74PCnCPKvj"
      },
      "source": [
        "### Exercise 1.1: Create the new index\n",
        "\n",
        "You can reuse the previous code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtLScqpdPKvj"
      },
      "source": [
        "ES_HOSTS = ['http://localhost:9200']\n",
        "EVAL_INDEX_NAME = 'government'\n",
        "EVAL_DOCS_PATH = 'practice_data/government/documents'\n",
        "\n",
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZQtSV6EPKvj"
      },
      "source": [
        "### Exercise 1.2. Read topics and produce result file\n",
        "\n",
        "Read topics (queries) from a file (`government/topics/gov.topics`) and then search documents indexed by **Elasticsearch**. You may choose one of search algorithms.\n",
        "\n",
        "Produce result file (e.g., *retrieved.txt*) according to **trec_eval** standard output format: \n",
        "\n",
        "`01 Q0 document1 0 1.23 my_IR_system1`\n",
        "\n",
        "`01 Q0 document2 1 1.08 my_IR_system1`\n",
        "\n",
        "where '01' is the query ID; ignore 'Q0'; 'documentX' is the name of the file; '0' (or '1' or some other integer number) is the rank of this result; '1.23' (or '1.08' or some other number) is the score of this result; and 'my_IR_system1' is the name for your retrieval system. In particular, note that the rank field will be ignored in **trec_eval**; internally ranks are assigned by sorting by the score field with ties broken deterministicly (using file name).\n",
        "\n",
        "**Now here's your first job**\n",
        "\n",
        "1. read `gov.topics` file line by line, \n",
        "2. send query to the elastic search\n",
        "3. write output according the the output format described above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dsgw-7wNPKvj"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGmZlewJPKvk"
      },
      "source": [
        "### Exercise 1.3.  Evaluation\n",
        "\n",
        "It's time to run **trec_eval** which compares the qrels file provided in *gov.qrels* with your result file. (hint: adding a **!** and shell commands allow you to execute shell commands in jupyter-notebook, e.g. `!ls`)\n",
        "\n",
        "TREC_EVAL will evaluate the performance of your search engine. To evaluate your search result, you first need two sets of files: the retrieved result file and the ground truth file.\n",
        "Let's say your retrieval result is saved at `retrieved.txt`, and the ground truth file is saved at `gov.qrels`. The performance of your retrieval can be measured via:\n",
        "\n",
        "```\n",
        "./trec_eval.9.0/trec_eval  gov.qrels retrieved.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFZK1yW4PKvl"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U57HRbcfPKvn"
      },
      "source": [
        "If **trec_eval** runs correctly and produces numbers which you think are sensible, you are done with this part. You might want to look at the output, though, and get some understanding of what it means; later you will be asked to interpret this and to choose evaluation measures you prefer.\n",
        "\n",
        "Running `./trec_eval.9.0/trec_eval -h` will list all the options available.\n",
        "\n"
      ]
    }
  ]
}