{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "language modelling.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acastellanos-ie/NLP-MBD-EN-BL-ENE-2021-J-1/blob/main/language_modelling/language%20modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCvwbQnTvBRh"
      },
      "source": [
        "# Google Colab Configuration\n",
        "\n",
        "**Execute this steps to configure the Google Colab environment in order to execute this notebook. It is not required if you are executing it locally and you have properly configured your local environment according to what explained in the Github Repository.**\n",
        "\n",
        "The first step is to clone the repository to have access to all the data and files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repository_name = \"NLP-MBD-EN-BL-ENE-2021-J-1\"\n",
        "repository_url = 'https://github.com/acastellanos-ie/' + repository_name"
      ],
      "metadata": {
        "id": "gakj_n0h70ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d7mC64KvlwP",
        "outputId": "e5cb877b-fee1-4be7-f1bf-343f2fba42a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! git clone $repository_url"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MBD-EN-BL-ENE-2020-J-1'...\n",
            "remote: Enumerating objects: 4481, done.\u001b[K\n",
            "remote: Counting objects: 100% (4481/4481), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4368/4368), done.\u001b[K\n",
            "remote: Total 4481 (delta 158), reused 4387 (delta 94), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (4481/4481), 13.41 MiB | 19.53 MiB/s, done.\n",
            "Resolving deltas: 100% (158/158), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecfec2Y4v6e9"
      },
      "source": [
        "Install the requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIep7l0jvtUB",
        "outputId": "d45eba5a-9325-44ed-ae93-ebf4493fa51a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! pip install -Uqqr $repository_name/requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.5MB 9.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.4MB 1.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 12.0MB 170kB/s \n",
            "\u001b[K     |████████████████████████████████| 9.9MB 42.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 348kB 44.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 49.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 727kB 24.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 454.3MB 35kB/s \n",
            "\u001b[K     |████████████████████████████████| 25.3MB 1.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 10.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.3MB 29.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 36.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 58.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 8.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 52.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0MB 33.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.0MB 26.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 471kB 53.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 44.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0MB 23.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 983kB 57.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 266kB 22.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 471kB 42.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 23.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 45.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.9MB 37.5MB/s \n",
            "\u001b[?25h  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for eli5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for stellargraph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for networkx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for syntok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement grpcio~=1.32.0, but you'll have grpcio 1.34.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement h5py~=2.10.0, but you'll have h5py 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorflow-estimator<2.5.0,>=2.4.0, but you'll have tensorflow-estimator 2.5.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 1.2.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: ktrain 0.26.2 has requirement scikit-learn==0.23.2, but you'll have scikit-learn 0.22.2.post1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: ktrain 0.26.2 has requirement transformers<=4.3.3,>=4.0.0, but you'll have transformers 4.6.0 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHDzMQwpyODo"
      },
      "source": [
        "Now you have everything you need to execute the code in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-qIO7sguzTC"
      },
      "source": [
        "# Language Modelling\n",
        "\n",
        "In this notebook we are going to start playing with languages models. In particular, we are going to start with the simplest approach based on n-grams. Then, in the following threads, we will move to more advanced approaches based on LSTM and Transformer architectures.\n",
        "\n",
        "The Natural Language Toolkit (NLTK) has data types and functions that make life easier for us when we want to count bigrams and compute their probabilities.\n",
        "\n",
        "Let's start!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIT1xuFUuzTK",
        "outputId": "8d62e73f-051c-472e-ce0b-a2540d446560"
      },
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcOKzXu1uzTL"
      },
      "source": [
        "**Import the Brown corpus**\n",
        "\n",
        "For the experimentation, we are going to use the well-known Brown Corpus.\n",
        "\n",
        "The Brown University Standard Corpus of Present-Day American Englis, or just Brown Corpus (https://en.wikipedia.org/wiki/Brown_Corpus),  is a general corpus containing 500 samples of English-language text, totaling roughly one million words, compiled from works published in the United States in 1961.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2SYcsnQuzTL",
        "outputId": "42f408c8-56ad-498d-cf2d-09c2465e91f6"
      },
      "source": [
        "from nltk.corpus import brown\n",
        "brown.categories()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adventure',\n",
              " 'belles_lettres',\n",
              " 'editorial',\n",
              " 'fiction',\n",
              " 'government',\n",
              " 'hobbies',\n",
              " 'humor',\n",
              " 'learned',\n",
              " 'lore',\n",
              " 'mystery',\n",
              " 'news',\n",
              " 'religion',\n",
              " 'reviews',\n",
              " 'romance',\n",
              " 'science_fiction']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9g71QARuzTM"
      },
      "source": [
        "From the words of the Brown corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoR82bqxuzTN",
        "outputId": "3d985cd8-87cd-4538-b752-e1fe63852877"
      },
      "source": [
        "brown.words()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNIIvekauzTN"
      },
      "source": [
        "Let's inspect what are the most likely (most frequent) words in the dataset. The probability of a word is very important for our language model. When we ask the LM to generate new text, it should rely on these word probabilities, so it can generate words that are likely in our dataset.\n",
        "\n",
        "We compute the word frequency by using the `FreqDist` function of NLTK (an nltk.FreqDist() is like a dictionary, but it is ordered by frequency)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq7nSgs0uzTN"
      },
      "source": [
        "The following uses this function to compute the freqs and plot the 20 most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ousstIaUuzTO",
        "outputId": "2d4cd3a0-307b-47a5-e86c-e35cdd720398"
      },
      "source": [
        "freq_brown = nltk.FreqDist(brown.words())\n",
        "\n",
        "list(freq_brown.keys())[:20]\n",
        "freq_brown.most_common(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 62713),\n",
              " (',', 58334),\n",
              " ('.', 49346),\n",
              " ('of', 36080),\n",
              " ('and', 27915),\n",
              " ('to', 25732),\n",
              " ('a', 21881),\n",
              " ('in', 19536),\n",
              " ('that', 10237),\n",
              " ('is', 10011),\n",
              " ('was', 9777),\n",
              " ('for', 8841),\n",
              " ('``', 8837),\n",
              " (\"''\", 8789),\n",
              " ('The', 7258),\n",
              " ('with', 7012),\n",
              " ('it', 6723),\n",
              " ('as', 6706),\n",
              " ('he', 6566),\n",
              " ('his', 6466)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N5aGMWyuzTO"
      },
      "source": [
        "We can see that they are mostly stopwords, punctuation signs.\n",
        "\n",
        "**Should we remove them? Why?** \n",
        "\n",
        "No, just think in what we are trying to do here. We are trying to use the dataset to create a model of the language to, given a set of words, predict the most probable next word. For this process, stopwords, as well as punctuation or other signs are need.\n",
        "\n",
        "For the same reason, we shall not stemmize/lemmatize, neither normalize the words. We need all these variations to learn a proper language model (i.e, `the` != `The`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHWVvOy8uzTP"
      },
      "source": [
        "## Bigram Model\n",
        "\n",
        "We'll start small and we will create a language model based on bi-grams. This LM is rather simplistic: it will only codify relationships of length 2.\n",
        "\n",
        "To that end, we will use the `ConditionalFreqDist` function of NLTK. `nltk.ConditionalFreqDist()` counts frequencies of pairs. When given a list of bigrams, it maps each first word of a bigram to a FreqDist over the second words of the bigram.\n",
        "\n",
        "If you remember the theoretical session, we are applying the Markov assumption: the next element (word in our case) of a sequence can be predicted by just focusing on the previous one.\n",
        "\n",
        "The following code creates these bi-gram counts.\n",
        "If we print the `conditions` we can see the antecedent of the bi-grams. (`conditions()` in a `ConditionalFreqDist` are like `keys()` in a dictionary)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLqSKRGluzTQ",
        "outputId": "a11070f3-4931-48b5-e829-3d8fd34b488c"
      },
      "source": [
        "cfreq_brown_2gram = nltk.ConditionalFreqDist(nltk.bigrams(brown.words()))\n",
        "cfreq_brown_2gram.conditions()[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'Fulton',\n",
              " 'County',\n",
              " 'Grand',\n",
              " 'Jury',\n",
              " 'said',\n",
              " 'Friday',\n",
              " 'an',\n",
              " 'investigation',\n",
              " 'of']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FyBBzaQuzTR"
      },
      "source": [
        "Let' see the most frequent terms after the word `my`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HZlMa8GuzTR",
        "outputId": "22ef2964-6869-4ee4-f34c-987f67ff0781"
      },
      "source": [
        "# the cfreq_brown_2gram entry for \"my\" is a FreqDist (i.e, a dictionary of word and freqCount).\n",
        "my_terms = cfreq_brown_2gram[\"my\"]\n",
        "\n",
        "# Sort the terms by frequency and print the 25th most common\n",
        "sorted(my_terms.items(), key=lambda x: -x[1])[:25]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('own', 52),\n",
              " ('hand', 19),\n",
              " ('life', 19),\n",
              " ('mind', 19),\n",
              " ('first', 15),\n",
              " ('wife', 14),\n",
              " ('hands', 14),\n",
              " ('eyes', 13),\n",
              " ('father', 13),\n",
              " ('mother', 12),\n",
              " ('husband', 12),\n",
              " ('way', 12),\n",
              " ('head', 11),\n",
              " ('left', 8),\n",
              " ('heart', 7),\n",
              " ('point', 7),\n",
              " ('body', 7),\n",
              " ('Uncle', 7),\n",
              " ('best', 6),\n",
              " ('family', 6),\n",
              " ('right', 6),\n",
              " ('brother', 6),\n",
              " ('friends', 6),\n",
              " ('name', 6),\n",
              " ('business', 6)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWvLg4L1uzTR"
      },
      "source": [
        "We can do the same with the `most_common` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eVkxT7DuzTS",
        "outputId": "2a0b04c3-5b85-45d2-fada-935e13da5ab8"
      },
      "source": [
        "cfreq_brown_2gram[\"my\"].most_common(25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('own', 52),\n",
              " ('hand', 19),\n",
              " ('life', 19),\n",
              " ('mind', 19),\n",
              " ('first', 15),\n",
              " ('wife', 14),\n",
              " ('hands', 14),\n",
              " ('eyes', 13),\n",
              " ('father', 13),\n",
              " ('mother', 12),\n",
              " ('husband', 12),\n",
              " ('way', 12),\n",
              " ('head', 11),\n",
              " ('left', 8),\n",
              " ('heart', 7),\n",
              " ('point', 7),\n",
              " ('body', 7),\n",
              " ('Uncle', 7),\n",
              " ('best', 6),\n",
              " ('family', 6),\n",
              " ('right', 6),\n",
              " ('brother', 6),\n",
              " ('friends', 6),\n",
              " ('name', 6),\n",
              " ('business', 6)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gtx3xuTuzTS"
      },
      "source": [
        "With the `nltk.ConditionalProbDist()`, map pairs are mapped to probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9Vk1D32uzTS"
      },
      "source": [
        "cprob_brown_2gram = nltk.ConditionalProbDist(cfreq_brown_2gram, nltk.MLEProbDist) # Uses a Maximum Likelihood Estimation (MLE) estimator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f8piubquzTS"
      },
      "source": [
        "This again has `conditions()` wihch are like dictionary keys"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeqSahgxuzTT",
        "outputId": "76409063-46ce-4fab-e79d-9239c68016f1"
      },
      "source": [
        "cprob_brown_2gram.conditions()[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'Fulton',\n",
              " 'County',\n",
              " 'Grand',\n",
              " 'Jury',\n",
              " 'said',\n",
              " 'Friday',\n",
              " 'an',\n",
              " 'investigation',\n",
              " 'of']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEIp1wo_uzTT"
      },
      "source": [
        "We can also find the words that can come after `my` by using the function `samples()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOBM_KLZuzTT",
        "outputId": "4a588369-6607-4b18-e5ef-e1b5575fed55"
      },
      "source": [
        "cprob_brown_2gram[\"my\"].samples()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['political', 'client', 'fellow', 'man', 'candidacy', 'best', 'place-kicking', 'last', 'reflexes', 'jobs', 'family', 'thanks', 'firm', 'payroll', 'judgment', 'sales', 'first', 'mother', 'boys', 'share', 'daily', 'wife', 'legs', 'big', 'hands', 'biologist', 'locker', 'hand', 'right', 'neck', 'heart', 'grudge', 'neighbor', 'brother', 'house', 'good', 'life', 'native', 'charge-a-plate', \"son's\", 'psychiatrist', 'son', 'children', 'arms', 'daughter', 'opinion', 'husband', 'friends', 'country', 'wonderful', 'school', 'home', 'desire', 'point', 'little', 'part', 'two', 'itinerary', 'classroom', 'initial', 'induction', 'own', 'students', 'classes', 'personal', 'only', 'estimation', 'taste', 'objectivity', 'bed', 'eyes', 'principal', 'primary', 'Roman', 'experience', 'stay', 'lot', 'leave', 'learned', 'Bible', 'nearest', 'Father', 'Saviour', 'patient', 'peace', 'work', 'patients', 'professional', 'talents', 'soul', 'light', 'salvation', 'foes', 'flesh', 'fingers', 'body', 'finger', 'word', 'lost', 'voice', 'name', '15th', 'pages', 'editorial', 'executive', 'early', 'aid', 'discouraged', 'roving', 'continued', 'three', 'father', 'turn', 'mind', 'desk', 'question', 'seedbed', ',', \"neighbors'\", 'morale', 'spirits', 'plants', 'SD', '37th', 'comments', 'double', 'scoped', 'tour', 'left', 'steps', 'privilege', 'subject', 'final', 'sketches', 'sketch', 'attack', 'studio', 'brushes', 'supposedly', 'machine', 'particular', 'girl', 'love', 'stomack', 'ink', 'Negroes', 'borough', 'second', 'thoughts', 'flashlight', 'room', 'feet', \"parents'\", 'collection', 'benefit', 'plans', 'neighbors', 'conscience', 'favorite', 'view', 'sacred', \"''\", 'being', 'information', 'usual', 'way', 'beautiful', 'need', 'liberal', \"wife's\", 'questions', 'conversations', 'binoculars', 'travel', 'dear', 'job', 'opening', 'statement', 'judge', 'cause', 'knowledge', 'actions', 'hotel', 'invention', 'plays', 'writing', 'world', 'knee', 'grandmother', 'head', 'reasons', 'feelings', 'companion', 'cold', 'burning', \"life's\", 'theme', 'employees', 'company', 'fellow-employees', 'business', 'age', 'secret', 'comrades', 'briefcase', 'money', 'recoil', 'psyche', 'bridegroom', 'fear', 'memory', 'wrongs', 'shames', 'discovery', 'intention', 'hope', 'innocence', 'general', 'trust', 'help', 'pupil', 'teaching', 'conviction', 'studied', 'honor', 'enthusiasm', 'city', 'participation', 'report', 'stories', 'state', 'derby', 'great', 'glasses', 'ability', 'goals', 'train', 'principles', 'recently', 'backpack', 'typewriter', 'regard', 'latest', 'end', '``', 'tent', 'case', 'entry', 'pallid', 'hesitation', 'visit', 'departure', 'position', 'projected', 'genius', 'destiny', 'silence', 'sad', \"song's\", 'churchgoing', 'sins', 'ultimate', 'shoulders', 'moral', 'religious', 'duty', \"father's\", \"mother's\", 'wishful', 'childishness', \"husband's\", 'L.', 'mare', 'Lorde', 'account', 'study', 'old', 'other', 'feeling', 'sons', 'books', 'Wall', 'bookshelves', 'endurance', 'votive', 'opinions', 'readers', 'youth', 'colleagues', 'attitudes', 'awareness', 'remarks', 'tendency', 'education', 'circle', 'generation', 'sentiments', 'sculptor', 'background', 'hopes', 'views', 'short', 'example', 'district', 'State', 'nine', 'esteem', 'constituents', 'newsletter', 'favorites', 'adopted', 'hunch', 'personality', 'fairly', 'failure', 'condescension', 'chair', 'Yokuts', 'full', 'remark', 'commitment', 'return', 'reactions', 'missing', 'ideal', 'honour', 'squad', 'senses', 'side', 'slovenliness', 'face', 'gun', 'flint', 'initiation', 'reality', 'recollection', 'amazement', 'shoulder', 'pants', 'arm', 'Lord', 'foot', 'child', 'people', 'pleasure', 'captain', 'crew', 'men', 'undershirt', 'clothes', 'greatest', 'sight', 'respectable', 'thirty', 'gray', 'weatherbeaten', 'vitals', 'place', 'encouragement', 'pass', 'landscape', 'list', 'kit', 'checkbook', 'regards', 'noise', 'throat', 'pot', 'hen', 'eighty-three', 'pocket', 'lungs', 'clotheshorse', 'present', 'dresser', 'scholarly', 'camp', 'street', 'mountain', 'existence', 'letters', 'imagination', 'bunkmate', 'fantasies', 'newest', 'dreams', 'adolescent', 'flashy', 'timidity', 'emotion', 'impending', 'closed', 'years', 'ear', 'wallet', 'medical', 'drinking', 'profession', 'military', 'seat', 'portrayal', 'bags', 'class', 'safe', 'change', 'ticket', 'nostrils', 'stomach', 'motel', 'evening', 'hair', 'car', 'cab', 'pockets', 'friend', 'twenty-first', 'regular', 'career', 'leg', 'office', 'references', 'trial', 'day', '275', 'road', 'watch', 'battered', 'pencil', 'Tim', 'pipe', 'forms', 'advice', 'small', 'presence', 'intelligence', 'line', 'unpadded', 'sister', '!', 'table', 'back', 'gapt', 'lessons', 'block', 'getting', 'dark', 'perennial', 'ward', 'guardian', 'property', 'mission', 'generator', 'brothers', 'elders', 'color', 'partner', 'boy', 'window', 'front', 'spare', 'rose', 'garden', 'post', 'shot', 'sights', 'knife', 'disappointment', 'frequent', 'attention', 'papers', 'past', 'dislike', 'visits', 'application', 'interview', 'knuckles', 'pardon', 'scholastic', 'motives', 'physical', 'spiritual', 'former', 'orders', 'time', 'stock', \"duds'd\", 'black', 'pilots', 'driver', 'latent', 'Spanish', 'host', \"host's\", 'supplicating', 'romantic', 'duffel', 'buying', 'shin', 'knees', 'camera', 'ears', 'kneeling', 'squatting', '(', 'astonished', 'thumb', 'elbows', 'focus', 'muffler', 'bell', 'robe', 'Colt', 'coat', 'shower', 'virtues', 'Uncle', 'normal', 'Aunt', 'new', 'looking', \"Uncle's\", 'lit', 'investment', 'jalopy', 'thighs', 'Anthropology', 'uncles', 'few', 'rosy', 'food', 'mough', 'guts', 'equipment', 'products', 'talks', 'brethren', 'fault', 'teeth', 'tongue', \"ever-lovin'\", 'meal', 'extra', 'contests', 'pin', 'yes', \"dryin'\", 'system', 'talking', 'game', 'bare', 'interest', 'skirt', 'finished', 'idea', 'paintings', 'pay', 'headlights', 'shopping', 'dentist', 'cottage', 'pity', 'very', 'person', 'fully', 'darling', 'town', 'shame', 'wish', 'promise', 'rock', 'stack', 'bureau', 'mood', 'lips', 'disquiet', 'private', 'protests', 'woman', 'dazzled', 'thinking', 'others', 'armor', 'frozen', 'topcoat', 'worries', 'purse', 'immense', 'empty', 'folks', 'faith', 'Johnnie', \"folks'\", 'truck', 'doing', 'strict', 'happiness', 'elbow', 'guest', 'release', 'watching', \"brother's\", 'pale', 'nephew', 'nerves', 'shorts', 'contacts', 'lawyer', 'God', 'consciousness', 'surprise', 'immediate', 'everyday', 'threats', 'souffle', 'diary', 'larder', 'lingerie', 'poor', 'art', 'hardest', 'unflagging', 'mouth', 'efforts', 'troubles', 'hurt', 'decision', \"daughter's\", 'chances', 'drink', 'uncle', 'dress', 'whole', 'lack', 'residence', 'dog'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8EPw4y0uzTT"
      },
      "source": [
        "In addition, you can see the prob of a particular pair"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fF3rUahluzTU",
        "outputId": "8e24f828-48b8-46be-b910-ff868d32efc5"
      },
      "source": [
        "cprob_brown_2gram[\"my\"].prob(\"own\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.04478897502153316"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPjydGm4uzTU",
        "outputId": "9a48860b-daa9-479a-f542-b46cc09ac3c4"
      },
      "source": [
        "cprob_brown_2gram[\"my\"].prob(\"leg\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0034453057708871662"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpzMKz1UuzTU"
      },
      "source": [
        "## Compute the probability of a sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXtPHiQ7uzTV"
      },
      "source": [
        "Create a function to compute the probability of a word from its frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Josl8K1uzTV",
        "outputId": "ff5cc192-40f7-420b-c9f0-3da4388fb02b"
      },
      "source": [
        "def unigram_prob(word):\n",
        "    len_brown = len(brown.words())\n",
        "    return float(freq_brown[word]) / float(len_brown)\n",
        "\n",
        "unigram_prob(\"night\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0003427512418273636"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc9Fa4-1uzTV"
      },
      "source": [
        "We now can ask for the probability of a word sequence.\n",
        "\n",
        "For instance: `P(how do you do) = P(how) * P(do|how) * P(you|do) * P(do | you)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12JoyG54uzTV",
        "outputId": "383cdfbf-3e0e-49c3-b292-658febb29682"
      },
      "source": [
        "unigram_prob(\"how\") * cprob_brown_2gram[\"how\"].prob(\"do\") * cprob_brown_2gram[\"do\"].prob(\"you\") * cprob_brown_2gram[\"you\"].prob(\"do\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5639033871961e-09"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkCsamOiuzTW"
      },
      "source": [
        "Compare it with the prob of another not so common sentence: `how do you dance`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI49ifcUuzTW",
        "outputId": "180791ef-f3a2-4a09-9524-83ade2fc964f"
      },
      "source": [
        "unigram_prob(\"how\") * cprob_brown_2gram[\"how\"].prob(\"do\") * cprob_brown_2gram[\"do\"].prob(\"you\") * cprob_brown_2gram[\"you\"].prob(\"dance\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0089699272232904e-10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSb3PvTruzTX"
      },
      "source": [
        "As expected, one order of magnitude less probable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWT8NGKkuzTX"
      },
      "source": [
        "## Generate Language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vJk7pcpuzTX"
      },
      "source": [
        "With our bi-gram language model already generated, we can now use it to generate text and see what has our model learned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XjCHMKxGuzTY",
        "outputId": "bd39e97d-70b5-466e-9483-813bbed3484b"
      },
      "source": [
        "cprob_brown_2gram[\"my\"].generate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'bed'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqZngBJyuzTY"
      },
      "source": [
        "Let's see if the model create valid text or just jiberish"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVS1qI2cuzTY",
        "outputId": "77c44234-eb30-444c-9dae-8ece90e087cd"
      },
      "source": [
        "word = \"my\"\n",
        "text = \"\"\n",
        "for index in range(20):\n",
        "    text += word + \" \"\n",
        "    word = cprob_brown_2gram[ word].generate()\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my place us forever . Next to be so many listeners , and now one of the meaning greetings from \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wdev8EPuzTZ"
      },
      "source": [
        "It is not a valid sentence, but it has some kind of sense. \n",
        "\n",
        "Remember that we are just learning from bigrams!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaZbER2euzTZ"
      },
      "source": [
        "**We can try another datasets to train a language models using different dataset.**\n",
        "\n",
        "In particular we are going to import the book dataset of NLTK, which includes the text of different books.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkaYqUZ5uzTZ"
      },
      "source": [
        "The following function takes a text (i.e., the text o a given book) to learn a language model, and a initial word to start the generation and the number of words that have to be generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3hpAVo9uzTZ",
        "outputId": "61418d6e-3835-4aaf-c02c-5e652e281a57"
      },
      "source": [
        "# Here is how to do this with NLTK books:\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('genesis')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('webtext')\n",
        "nltk.download('treebank')\n",
        "\n",
        "from nltk.book import *\n",
        "\n",
        "\n",
        "def generate_text(text, initialword, numwords):\n",
        "    bigrams = list(nltk.ngrams(text, 2))\n",
        "    cpd = nltk.ConditionalProbDist(nltk.ConditionalFreqDist(bigrams), nltk.MLEProbDist)\n",
        "\n",
        "    word = initialword\n",
        "    text = \"\"\n",
        "    for i in range(numwords):\n",
        "        text += word + \" \"\n",
        "        word = cpd[ word].generate() \n",
        "\n",
        "    print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVhYqc5HuzTa"
      },
      "source": [
        "We use different books to generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVD06Gf8uzTa",
        "outputId": "9f0fbd7e-24f2-44c1-b6c2-ae160ceb871e"
      },
      "source": [
        "# Holy Grail\n",
        "generate_text(text6, \"I\", 25)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am Zoot ! MINSTREL : Patsy . Come on ! It is your mother ! Go away , as soon to make them an \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXk0K-vzuzTa"
      },
      "source": [
        "# sense and sensibility\n",
        "generate_text(text2, \"I\", 25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4s6o2hHuzTa"
      },
      "source": [
        "# TriGrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssCJc1-iuzTb"
      },
      "source": [
        "Let's try a more advance model using tri-grams to see if it is able to generate better language.\n",
        "\n",
        "We cannot use the `ConditionalFreqDist` as before. `nltk.ConditionalFreqDist` expects its data as a sequence of `(condition, item)` tuples. `nltk.trigrams` returns tuples of length 3. Therefore, we have to adapt the trigrams output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRZ9Rm1NuzTb"
      },
      "source": [
        "def generate_text(text, initialword, numwords):\n",
        "    trigrams = list(nltk.ngrams(text, 3,  pad_right=True, pad_left=True))\n",
        "    trigram_pairs = (((w0, w1), w2) for w0, w1, w2 in trigrams) # Adapt the format to use ConditionalFreqDist\n",
        "    cpd = nltk.ConditionalProbDist(nltk.ConditionalFreqDist(trigram_pairs), nltk.MLEProbDist)\n",
        "\n",
        "    word = initialword\n",
        "    text = \"\"\n",
        "    for i in range(numwords):\n",
        "        w = cpd[(word[i],word[i+1])].generate() \n",
        "        word += [w]\n",
        "    \n",
        "    print(\" \".join(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxWLWg55uzTb",
        "outputId": "272b8e61-82d9-451c-aaf5-0209328e2840"
      },
      "source": [
        "generate_text(text2, [\"I\", \"am\"], 25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am so sorry we cannot stay here long , and put on a twilight walk to the bustle , and of whose success he was affected\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsDDHFXPuzTc"
      },
      "source": [
        "As expected, it creates a better lm.\n",
        "\n",
        "Can we go on with more n-grams? Let's see"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Y1TnU7uzTc"
      },
      "source": [
        "# N-grams\n",
        "\n",
        "We are going to update again the `generate_text` function to create a language model based on 4-grams.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI4GDN_JuzTc"
      },
      "source": [
        "def generate_text(text, initialword, numwords):\n",
        "    ngrams = list(nltk.ngrams(text, 4,  pad_right=True, pad_left=True))\n",
        "    ngram_pairs = (((w0, w1, w2), w3) for w0, w1, w2, w3 in ngrams)\n",
        "    cpd = nltk.ConditionalProbDist(nltk.ConditionalFreqDist(ngram_pairs), nltk.MLEProbDist)\n",
        "\n",
        "    word = initialword\n",
        "    text = \"\"\n",
        "    for i in range(numwords):\n",
        "        w = cpd[(word[i],word[i+1], word[i+2])].generate() \n",
        "        word += [w]\n",
        "    \n",
        "    print(\" \".join(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIcg_O0_uzTc",
        "outputId": "618fb217-f716-48c8-cdd4-dfb42b2d867b"
      },
      "source": [
        "generate_text(text2, [\"I\", \"am\", \"very\"], 25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am very sure that Colonel Brandon has not the smallest chance . As soon , however , towards that unfortunate girl -- I must say it ,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBjSpnNwuzTd"
      },
      "source": [
        "As we make the n-grams larger we got more accurate language models. However, as explained in class, if we create large n-grams we are not going to have enough data to train our models: we will never see enough data (enough sequences of n-grams) to train the model.\n",
        "\n",
        "As an exercise, I leave up to you to keep extending this LM model to 5-gram, 6-gram....\n"
      ]
    }
  ]
}